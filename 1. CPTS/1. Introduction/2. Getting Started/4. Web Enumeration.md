
| **Tools** | **Command**                                                                                  | **Usage**                                     |
| --------- | -------------------------------------------------------------------------------------------- | --------------------------------------------- |
| GoBuster  | gobuster dir -u http://10.10.10.121/ -w /usr/share/seclists/Discovery/Web-Content/common.txt | To perform this directory enumeration         |
|           | gobuster dns -d inlanefreight.com -w /usr/share/SecLists/Discovery/DNS/namelist.txt          | Run a sub-domain scan on a website            |
| Curl      | curl -IL https://www.inlanefreight.com                                                       | Grab website banner                           |
|           | curl 10.10.10.121/robots.txt                                                                 | List potential directories in `robots.txt`    |
| whatweb   | whatweb --no-errors 10.10.10.121                                                             | List details about the webserver/certificates |
## Directory Enumeration
We can uncover any hidden files or directories on the webserver that are not intended for public access. We can use a tool such as [ffuf](https://github.com/ffuf/ffuf) or [GoBuster](https://github.com/OJ/gobuster) to perform this directory enumeration.

## DNS Subdomain Enumeration
There also may be essential resources hosted on subdomains, such as admin panels or applications with additional functionality that could be exploited. We can use `GoBuster` to enumerate available subdomains of a given domain using the `dns` flag to specify DNS mode. First, let us clone the SecLists GitHub [repo](https://github.com/danielmiessler/SecLists), which contains many useful lists for fuzzing and exploitation:
```shell
git clone https://github.com/danielmiessler/SecLists

sudo apt install seclists -y
```
Next, add a DNS Server such as 1.1.1.1 to the `/etc/resolv.conf` file. We will target the domain `inlanefreight.com`, the website for a fictional freight and logistics company.
```shell
gobuster dns -d inlanefreight.com -w /usr/share/SecLists/Discovery/DNS/namelist.txt
```

We can use `cURL` to retrieve server header information from the command line.
## Robots.txt
It is common for websites to contain a `robots.txt` file, whose purpose is to instruct search engine web crawlers such as Googlebot which resources can and cannot be accessed for indexing. The `robots.txt` file can provide valuable information such as the location of private files and admin pages. In this case, we see that the `robots.txt` file contains two disallowed entries.
![robots](https://github.com/user-attachments/assets/340c94ad-cbc7-4992-a36d-44eebf6963b9)


## Source Code
This example reveals a developer comment containing credentials for a test account, which could be used to log in to the website.
![source](https://github.com/user-attachments/assets/3c3bacd7-285b-4da7-81d3-66dc7d333291)

